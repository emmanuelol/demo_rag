services:
  ollama:
    image: ollama/ollama
    container_name: ollama # Sets the container name to ollama
    ports:
      - "11434:11434" # Maps port 11434 on the host to port 11434 in the container
    #  - "11435:11435"
    #environment:
    #  - OLLAMA_HOST=127.0.0.1:11435
    volumes:
      - ./entrypoint.sh:/entrypoint.sh
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia # Specifies the driver to use for GPU
              count: 1 # Reserves 1 GPU
              capabilities: [gpu] # Specifies that the device capability is GPU
    #command: 'ollama pull deepseek-r1:1.5b && ollama serve'
    tty: true
    entrypoint: ["/usr/bin/bash", "/entrypoint.sh"]
  client:
    build:
      context: ./client # Specifies the build context as the ./app directory
      dockerfile: Dockerfile # Uses the Dockerfile in the build context
    working_dir: /app
    volumes:
      - ./:/app
      - ./entrypoint_rag.sh:/entrypoint_rag.sh
      - /media/Datos/datasets:/datasets
      - /media/Datos/models:/models
    environment:
      BASE_URL: http://ollama:11434
      GRADIO_SERVER_NAME: 0.0.0.0
      GRADIO_SERVER_PORT: 7860
    ports:
      - "8501:8501"
      - "7680:7680"
    depends_on:
      - ollama
    tty: true 
    #entrypoint: /bin/sh python run_rag.py
    entrypoint: ["/usr/bin/bash", "/entrypoint_rag.sh"]
    #command: ["python", "/app/run_rag.py"]
    #healthcheck:
      #test:  [ "CMD-SHELL", "bash", "-c", "{ printf >&3 'GET / HTTP/1.0\\r\\n\\r\\n'; cat <&3; } 3<>/dev/tcp/ollama/11434 | grep 'Ollama is' || exit 1"  ] 
      #test: ["CMD-SHELL", "curl -f http://ollama:11434 || exit 1"]
      #test: ["CMD-SHELL", "curl -f http://ollama:11434 "]
      #interval: 20s
      #timeout: 5s
      #retries: 3
      #start_period: 10s
    
    




