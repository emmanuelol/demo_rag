{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f7ed77d-eee8-4d77-ad1a-2801e4ec1eba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain_community.document_loaders import PyMuPDFLoader,PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "import ollama\n",
    "import re\n",
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce05a044-52c9-40ff-998c-03acdcbf8cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/site-packages (5.2.0)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/site-packages (from pypdf) (4.12.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0baa358a-9131-4f13-b8a9-2217d5fcb68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml','r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c8c4c03-fe68-4e46-9c47-5fc486411381",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_embedding = config['models']['embedding']\n",
    "embeddings_directory = config['path']['emmbeddings']\n",
    "pdf_bytes = config['path']['pdfs']\n",
    "create_embeddings = config['parameters']['create_embeddings']\n",
    "chunk_size = config['parameters']['chunk_size']\n",
    "chunk_overlap = config['parameters']['chunk_overlap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8680edc3-f48c-4bbb-96d0-5ba2f60e0afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "question ='please provide me the structure of a docker-compose.yaml file'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1857227e-f748-4a34-ae10-cfc6c7ea4a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_bytes, model_embedding, persist_directory):\n",
    "    if pdf_bytes is None:\n",
    "        return None, None, None\n",
    "\n",
    "    #loader = PyMuPDFLoader(pdf_bytes)\n",
    "    loader = PyPDFDirectoryLoader(pdf_bytes)\n",
    "    data = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500, chunk_overlap=100\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "\n",
    "    embeddings = OllamaEmbeddings(model=model_embedding)\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=chunks, embedding=embeddings, persist_directory=persist_directory\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    return text_splitter, vectorstore, retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "638be760-be28-4fcc-96bb-60ba969f2e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(model_embedding, persist_directory, chunk_size, chunk_overlap):\n",
    "    \n",
    "    vectordb = Chroma(persist_directory=persist_directory,\n",
    "                      embedding_function= OllamaEmbeddings(model=model_embedding))\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "         chunk_size=chunk_size, \n",
    "         chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    \n",
    "    retriever = vectordb.as_retriever()\n",
    "\n",
    "    \n",
    "    return text_splitter, vectordb, retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c9586b0-876e-436d-84d0-8877321bde67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b32c5b7-b7eb-4188-9f8f-0449ec0348df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ollama_llm(question, context, model_embedding):\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=model_embedding,\n",
    "        messages=[{\"role\": \"user\", \"content\": formatted_prompt}],\n",
    "    )\n",
    "\n",
    "    response_content = response[\"message\"][\"content\"]\n",
    "\n",
    "    # Remove content between <think> and </think> tags to remove thinking output\n",
    "    final_answer = re.sub(r\"<think>.*?</think>\", \"\", response_content, flags=re.DOTALL).strip()\n",
    "\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0484f02-96ee-478b-8f22-65d8eba64568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chain(question, text_splitter, retriever, model_embedding):\n",
    "    \n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    \n",
    "    formatted_content = combine_docs(retrieved_docs)\n",
    "    \n",
    "    return ollama_llm(question, formatted_content, model_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0530d6fa-4eaf-4802-afde-9f319c8584a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(pdf_bytes, question, create_embeddings, embeddings_directory, model_embedding, chunk_size, chunk_overlap):\n",
    "    if create_embeddings is True:\n",
    "        text_splitter, vectorstore, retriever = process_pdf(pdf_bytes,\n",
    "                                                            embeddings_directory,\n",
    "                                                            chunk_size,\n",
    "                                                            chunk_overlap\n",
    "                                                           )\n",
    "    else:\n",
    "        text_splitter, vectorstore, retriever = load_embeddings(model_embedding,\n",
    "                                                                embeddings_directory,\n",
    "                                                                chunk_size,\n",
    "                                                                chunk_overlap\n",
    "                                                               )\n",
    "\n",
    "    if embeddings_directory is None:\n",
    "        return None  # No PDF uploaded\n",
    "\n",
    "    result = rag_chain(question, \n",
    "                       text_splitter,  \n",
    "                       retriever, \n",
    "                       model_embedding)\n",
    "    \n",
    "    return {result}\n",
    "    #return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8f492d1-5041-4cd7-968a-4924c52b461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ask_question(pdf_bytes ,question,create_embeddings, embeddings_directory, model_embedding, chunk_size, chunk_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3aae8977-d0f5-4415-91e3-6388a66f896c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Here\\'s an example of a `docker-compose.yaml` file for a machine learning pipeline using pandas and ydata_profiling:\\n\\n```yaml\\nservices:\\n  data-loader:\\n    environment: python324\\n    ports:\\n      - \"10800:10800\"\\n    name: Data loader service\\n    description: Loads CSV data for EDA\\n    arguments:\\n      - python324: $input_path\\n    parameters:\\n      - input_path: String\\n\\n  ydata_profiling_server:\\n    environment: python324\\n    ports:\\n      - \"10800:10800\"\\n    name: YData Profiling Server\\n    description: Runtime service for generating EDA report\\n    arguments:\\n      - $input_path: String\\n\\n  report-generator:\\n    environment: python324\\n    ports:\\n      - \"10800:10800\"\\n    name: Report Generator\\n    description: Exports EDA report to HTML\\n    parameters:\\n      - output_path: String\\n\\n  scheduler:\\n    context: root_context\\n    arguments:\\n      - \"data-loader $ydata_profiling_server report-generator\"\\n\\n  start_from: root_context\\nendservices\\n\\ndocker-compose up --build\\n```\\n\\n### Explanation:\\n\\n1. **Services**:\\n   - `data-loader`: A service that reads a CSV file from an input path.\\n   - `ydata_profiling_server`: The runtime service that performs the EDA using ydata_profiling.\\n   - `report-generator`: A service that exports the EDA report to HTML.\\n\\n2. **Containers and Context**:\\n   - Services are defined in the root container context.\\n   - The `scheduler` task starts services after `data-loader` initializes, ensuring all services start in order.\\n\\n3. **Imports and Runtime Profiles**:\\n   - All necessary libraries (pandas, ydata_profiling) are imported at the root level to ensure they\\'re available when the container runs.\\n   - Both `ydata_profiling_server` and `report-generator` use `ServerProfile`, which is recommended for runtime dependencies that should run in the background.\\n\\n4. **Port Usage**:\\n   - Uses port 10800 for multiple containers, allowing services to coexist efficiently.\\n\\nThis structure ensures a clean and efficient containerized pipeline for EDA using pandas and ydata_profiling.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3f9da24-640c-4a58-8d2f-32955b763e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74838944-db7d-4553-a928-b9a1809ed080",
   "metadata": {},
   "source": [
    "'Here\\'s an example of a `docker-compose.yaml` file for a machine learning pipeline using pandas and ydata_profiling:\\n\\n```yaml\\nservices:\\n  data-loader:\\n    environment: python324\\n    ports:\\n      - \"10800:10800\"\\n    name: Data loader service\\n    description: Loads CSV data for EDA\\n    arguments:\\n      - python324: $input_path\\n    parameters:\\n      - input_path: String\\n\\n  ydata_profiling_server:\\n    environment: python324\\n    ports:\\n      - \"10800:10800\"\\n    name: YData Profiling Server\\n    description: Runtime service for generating EDA report\\n    arguments:\\n      - $input_path: String\\n\\n  report-generator:\\n    environment: python324\\n    ports:\\n      - \"10800:10800\"\\n    name: Report Generator\\n    description: Exports EDA report to HTML\\n    parameters:\\n      - output_path: String\\n\\n  scheduler:\\n    context: root_context\\n    arguments:\\n      - \"data-loader $ydata_profiling_server report-generator\"\\n\\n  start_from: root_context\\nendservices\\n\\ndocker-compose up --build\\n```\\n\\n### Explanation:\\n\\n1. **Services**:\\n   - `data-loader`: A service that reads a CSV file from an input path.\\n   - `ydata_profiling_server`: The runtime service that performs the EDA using ydata_profiling.\\n   - `report-generator`: A service that exports the EDA report to HTML.\\n\\n2. **Containers and Context**:\\n   - Services are defined in the root container context.\\n   - The `scheduler` task starts services after `data-loader` initializes, ensuring all services start in order.\\n\\n3. **Imports and Runtime Profiles**:\\n   - All necessary libraries (pandas, ydata_profiling) are imported at the root level to ensure they\\'re available when the container runs.\\n   - Both `ydata_profiling_server` and `report-generator` use `ServerProfile`, which is recommended for runtime dependencies that should run in the background.\\n\\n4. **Port Usage**:\\n   - Uses port 10800 for multiple containers, allowing services to coexist efficiently.\\n\\nThis structure ensures a clean and efficient containerized pipeline for EDA using pandas and ydata_profiling.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fbc721-4d21-4d21-beb5-46d4738fc233",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
